---
title: "final_exploratoryanalysis"
author: "Kevin P. Patterson"
date: "2022-11-13"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(ggplot2)
library(patchwork)
library(dplyr)
library(readxl)
library(tidycensus)
library(rvest)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

```

```{r load data}
data2008 = read_excel((file = "data/annual_data2008.xlsx"), col_types = "text")
data2010 = read_excel((file = "data/annual_data2010.xlsx"), col_types = "text") 
data2012 = read_excel((file = "data/annual_data2012.xlsx"), col_types = "text") 
data2013 = read_excel((file = "data/annual_data2013.xlsx"), col_types = "text") 
data2014 = read_excel((file = "data/annual_data2014.xlsx"), col_types = "text")
data2015 = read_excel((file = "data/annual_data2015.xlsx"), col_types = "text") 
data2016 = read_excel((file = "data/annual_data2016.xlsx"), col_types = "text")
data2017 = read_excel((file = "data/annual_data2017.xlsx"), col_types = "text") 
data2018 = read_excel((file = "data/annual_data2018.xlsx"), col_types = "text")
data2019 = read_excel((file = "data/annual_data2019.xlsx"), col_types = "text")
data2020 = read_excel((file = "data/annual_data2020.xlsx"), col_types = "text")
    
  menustat_df = bind_rows(data2008, data2010, data2012, data2013, data2014, data2015, data2016, data2017, data2018, data2019, data2020) %>% 
  janitor::clean_names() %>% 
    mutate( 
    year = as.numeric(year), 
    serving_size = as.numeric(serving_size), 
    calories = as.numeric(calories), 
    total_fat = as.numeric(total_fat), 
    saturated_fat = as.numeric(saturated_fat), 
    trans_fat = as.numeric(trans_fat), 
    cholesterol = as.numeric(cholesterol),
    sodium = as.numeric(sodium), 
    potassium = as.numeric(potassium), 
    carbohydrates = as.numeric(carbohydrates), 
    protein = as.numeric(protein), 
    sugar = as.numeric(sugar),
    dietary_fiber = as.numeric(dietary_fiber),
    calories_100g = as.numeric(calories_100g),
    total_fat_100g = as.numeric(total_fat_100g), 
    saturated_fat_100g = as.numeric(saturated_fat_100g), 
    trans_fat_100g = as.numeric(trans_fat_100g), 
    cholesterol_100g = as.numeric(cholesterol_100g),
    sodium_100g = as.numeric(sodium_100g), 
    potassium_100g = as.numeric(potassium_100g), 
    carbohydrates_100g = as.numeric(carbohydrates_100g), 
    protein_100g = as.numeric(protein_100g), 
    sugar_100g = as.numeric(sugar_100g), 
    dietary_fiber_100g = as.numeric(dietary_fiber_100g)
      ) %>% 
    mutate(rest_type = 
             case_when(restaurant == "Culver's" | restaurant == "Subway" | restaurant == "Tim Hortons" | restaurant == "Taco Bell" | restaurant == "Arby's" | restaurant == "Whataburger" | restaurant == "White Castle" | restaurant == "Bojangles" | restaurant == "Burger King" | restaurant == "Captain D's" | restaurant == "Carl's Jr." | restaurant == "Chick-Fil-A" | restaurant == "Chipotle" | restaurant == "Church's Chicken" | restaurant == "Ci Ci's Pizza" | restaurant == "Dairy Queen" | restaurant == "Del Taco" | restaurant == "Dominos" | restaurant == "Einstein Bros" | restaurant == "El Pollo Loco" | restaurant == "Hardee's" | restaurant == "In-N-Out Burger" | restaurant == "Jack in the Box" | restaurant == "Jason's Deli" | restaurant == "KFC" | restaurant == "Krystal" | restaurant == "Little Caesars" | restaurant == "Long John Silver's" | restaurant == "McDonald's" | restaurant == "Panda Express" | restaurant == "Panera Bread" | restaurant == "Papa John's" | restaurant == "Papa Murphy's" | restaurant == "Pizza Hut" | restaurant == "Popeyes" | restaurant == "Quiznos" | restaurant == "Sonic" | restaurant == "Wendy's" | restaurant == "Zaxby's" | restaurant == "Checker's Drive-In/Rallys" | restaurant == "Five Guys" | restaurant == "Qdoba" | restaurant == "Sbarro" | restaurant == "Auntie Anne's" | restaurant == "Firehouse Subs" | restaurant == "McAlister's Deli" | restaurant == "Moe's Southwest Grill" | restaurant == "Wingstop" | restaurant == "Frisch's Big Boy" | restaurant == "Potbelly Sandwich Shop" | restaurant == "Freddy's Frozen Custard & Steakburgers" | restaurant == "Raising Cane's Chicken Fingers" | restaurant == "Shake Shack" | restaurant ==  "Jimmy John's" | restaurant == "Jersey Mike's" | restaurant ==  "Noodles & Company" | restaurant == "Portillo's" | restaurant == "Marco's Pizza" ~ "fast food", restaurant == "Friendly's" | restaurant == "Romano's Macaroni Grill" | restaurant == "Applebee's" | restaurant == "Bob Evans" | restaurant == "Boston Market" | restaurant == "Chili's" | restaurant == "Denny's" | restaurant == "Golden Corral" | restaurant == "O'Charley's" | restaurant == "Olive Garden" | restaurant == "PF Chang's" | restaurant == "Red Lobster" | restaurant == "Ruby Tuesday" | restaurant == "Steak 'N Shake" | restaurant == "California Pizza Kitchen" | restaurant == "IHOP" | restaurant == "LongHorn Steakhouse" | restaurant == "Outback Steakhouse" | restaurant == "TGI Friday's" | restaurant == "Carrabba's Italian Grill" | restaurant == "Chuck E. Cheese" | restaurant == "On the Border" | restaurant == "Perkins" | restaurant == "Red Robin" | restaurant == "Bonefish Grill" | restaurant == "Yard House" | restaurant == "Joe's Crab Shack" | restaurant == "BJ's Restaurant & Brewhouse" | restaurant == "Famous Dave's" | restaurant == "Round Table Pizza" | restaurant == "Hooters" | restaurant == "The Capital Grille" | restaurant == "Dickey's Barbeque Pit" | restaurant == "Buffalo Wild Wings" |restaurant == "Cheddar's Casual Café/Cheddar's Scratch Kitchen" | restaurant == "Cracker Barrel" | restaurant == "Dave & Buster's" | restaurant == "Logans Roadhouse" | restaurant == "The Cheesecake Factory" | restaurant == "Waffle House" ~ "sit-down", restaurant == "Starbucks" | restaurant == "Baskin Robbins" | restaurant == "Dunkin' Donuts" | restaurant == "Jamba Juice" | restaurant == "Krispy Kreme" | restaurant == "Tropical Smoothie Café" ~ "beverages_dessert", restaurant == "7 Eleven" | restaurant == "Casey's General Store" | restaurant == "Sheetz" | restaurant == "Wawa" ~ "convenience")) %>% 
    mutate(rest_type = as.factor(rest_type)) %>% 
    select(restaurant, rest_type, year, menu_item_id, food_category, item_name, item_description, serving_size, serving_size_unit, everything()) %>% 
    drop_na(calories, total_fat, sodium, carbohydrates, sugar)
## Dropping NAs took us from 274005 obs to 217653 obs 
  
  summary(menustat_df)

## Kevin's attempts (Rae also tried writing functions and couldn't get it to work)
## menustat_df = 
 # tibble(
  #  files = list.files("data/"), #listing the "./" method due to inability to read with "/data" alone
 #   path = str_c("data/", files)
 # ) %>% 
 # mutate(data = purrr::map(path, readxl::read_excel)) %>% 
 # unnest()

#menustat_df <- list.files(path = "data/", pattern = "*.xlsx", full.names = TRUE) %>% 
 # map_dfr(
 # .x = menustat_df,
  #.f = ~read_menu_stat(.x)
  # )

#menustat_lapply = 
#  list.files(path = "data/") %>% 
 # lapply(read_excel)  %>% 
 # bind_rows()
```

## Restaurant inspections data

```{r}
restaurant_address = read_csv('data/Restaurants__rolled_up_.csv') %>% 
  janitor::clean_names() %>% 
  filter(
    dba == "Culver's" | dba == "SUBWAY" | dba == "TIM HORTONS" | dba == "TACO BELl" | dba == "ARBY'S" | dba == "Whataburger" | dba == "WHITE CASTLE" | dba == "Bojangles" | dba == "BURGER KING" | dba == "Captain D's" | dba == "Carl's Jr." | dba == "CHICK-FIL-A" | dba == "CHIPOTLE MEXICAN GRILL" | dba == "Church's Chicken" | dba == "Ci Ci's Pizza" | dba == "DAIRY QUEEN GRILL & CHILL" | dba == "DEL TACO" | dba == "DOMINOS" | dba == "Einstein Bros" | dba == "EL POLLO LOCO" | dba == "HARDEE" | dba == "IN-N-OUT" | dba == "Jack in the Box" | dba == "Jason's Deli" | dba == "KFC" | dba == "Krystal" | dba == "LITTLE CAESARS" | dba == "Long John Silver's" | dba == "McCDONALD'S" | dba == "PANDA EXPRESS" | dba == "PANERA BREAD" | dba == "PAPA JOHN'S" | dba == "Papa Murphy's" | dba == "[Pp][Ii][Zz][Zz][Aa] [Hh][Uu][Tt]" | dba == "POPEYES" | dba == "[Pp][Oo][Pp][Ee][Yy][Ee]'[Ss]" | dba == "Quiznos" | dba == "SONIC" | dba == "WENDY'S" | dba == "Zaxby's" | dba == "CHECKERS" | dba == "FIVE GUYS FAMOUS BURGERS AND FRIES" | dba == "QDOBA" | dba == "SBARRO" | dba == "[Aa][Uu][Nn][Tt][Ii][Ee] [Aa][Nn][Nn][Ee]'[Ss]" | dba == "Firehouse Subs" | dba == "McAlister's Deli" | dba == "MOE'S SOUTHWEST GRILL" | dba == "[Ww][Ii][Nn][Gg][Ss][Tt][Oo][Pp]" | dba == "Frisch's Big Boy" | dba == "POTBELLY SANDWICH" | dba == "FREDDY'S" | dba == "Raising Cane's Chicken Fingers" | dba == "SHAKE SHACK" | dba == "JIMMY JOHNS" | dba == "[Jj][Ee][Rr][Ss][Ee][Yy] [MM][Ii][Kk][Ee]'[Ss]" | dba == "Noodles & Company" | dba == "Portillo's" | dba == "MARCO'S" | dba == "Friendly's" | dba == "Romano's Macaroni Grill" | dba == "APPLEBEE'S" | dba == "APPLEBEES" | dba == "Bob Evans" | dba == "BOSTON MARKET" | dba == "CHILI'S" | dba == "Denny's" | dba == "Golden Corral" | dba == "O'Charley's" | dba == "Olive Garden" | dba == "P.F.CHANG'S" | dba == "RED LOBSTER" | dba == "Ruby Tuesday" | dba == "Steak 'N Shake" | dba == "CALIFORNIA PIZZA KITCHEN" | dba == "IHOP" | dba == "LONGHORN STEAKHOUSE" | dba == "OUTBACK STEAKHOUSE" | dba == "TGI FRIDAY'S" | dba == "Carrabba's Italian Grill" | dba == "CHUCK E. CHEESE'S" | dba == "On the Border" | dba == "PERKINS RESTAURANT & BAKERY" | dba == "Red Robin" | dba == "Bonefish Grill" | dba == "YARD HOUSE" | dba == "Joe's Crab Shack" | dba == "BJ's Restaurant & Brewhouse" | dba == "Famous Dave's" | dba == "Round Table Pizza" | dba == "HOOTERS" | dba == "THE CAPITAL GRILLE" | dba == "DICKEY'S BARBEQUE PIT" | dba == "BUFFALO WILD WINGS" |dba == "Cheddar's Casual Café/Cheddar's Scratch Kitchen" | dba == "Cracker Barrel" | dba == "DAVE & BUSTER'S" | dba == "Logans Roadhouse" | dba == "THE CHEESECAKE FACTORY" | dba == "WAFFLE HOUSE" | dba == "STARBUCKS COFFEE" | dba == "BASKIN ROBBINS" | dba == "DUNKIN’" | dba == "JAMBA JUICE" | dba == "KRISPY KREME" | dba == "TROPICAL SMOOTHIE CAFE") 
    
```


## Census Data 

```{r}
# Set API key, only need to do this once but 'overwrite' will allow multiple
census_api_key("0b1055607d9ae137c0e00de8e22ed531d7c91a44", overwrite = TRUE, install = TRUE)

# run to use 
readRenviron("~/.Renviron")
#calls API key 
Sys.getenv("CENSUS_API_KEY")

# Call ist of all variables for ACS
vars_list = load_variables(year = 2020, 
                           dataset = "acs5", 
                           cache = TRUE) %>% 
            view()

# B19013_001 median household income
# B01003_001 total population
# B02001_001 total population (for race)
# B02001_002 white alone
# B02001_003 black alone
# B02001_004 american indian/alaskan native 
# B02001_005 asian alone
# B02001_006 native hawaiian and other pacific islander 
# B02001_007 Some other race alone

# Get data 2016-2020 for NYC by census tract 
nyc_acs_data <- get_acs(state = "ny",
               geography = "tract",
               year = 2020,
               variables = c(med_income = "B19013_001",
                             total_pop = "B01003_001", 
                             race_total = "B02001_001", 
                             white = "B02001_002", 
                             black_aa = "B02001_003", 
                             ai_an = "B02001_004", 
                             asain = "B02001_005", 
                             hawaiian_pi = "B02001_006", 
                             other = "B02001_007" 
                             ),
               geometry = TRUE,
               survey = "acs5") %>% 
  filter(str_detect(NAME, c("Bronx County", "Kings County", "Richmond County", "Queens County", "New York County"))) 
```

**ACS individual year pull, no function**
```{r ACS for one year pull}
library(tidyverse)
library(tidycensus)
library(sf)
library(scales)
#2016
acs_ny_2016 <- get_acs(
  geography = "zcta", 
  variables = c(med_income = "B19013_001",
                total_pop = "B01003_001", 
                race_total = "B02001_001", 
                white = "B02001_002", 
                black_aa = "B02001_003", 
                ai_an = "B02001_004", 
                asian = "B02001_005", 
                hawaiian_pi = "B02001_006", 
                other = "B02001_007"
                ),
  survey = "acs5", #pulls data from 2016-2020 so we have a cross section of 5 years
  year = 2016
) %>%
  filter(str_detect(GEOID, "^36")) %>%
  tibble::add_column(year = 2016)
  
#2017
acs_ny_2017 <- get_acs(
  geography = "zcta", 
  variables = c(med_income = "B19013_001",
                total_pop = "B01003_001", 
                race_total = "B02001_001", 
                white = "B02001_002", 
                black_aa = "B02001_003", 
                ai_an = "B02001_004", 
                asian = "B02001_005", 
                hawaiian_pi = "B02001_006", 
                other = "B02001_007"
                ),
  survey = "acs5", #pulls data from 2016-2020 so we have a cross section of 5 years
  year = 2017
) %>%
  filter(str_detect(GEOID, "^36")) %>%
  tibble::add_column(year = 2017)
#2018
acs_ny_2018 <- get_acs(
  geography = "zcta", 
  variables = c(med_income = "B19013_001",
                total_pop = "B01003_001", 
                race_total = "B02001_001", 
                white = "B02001_002", 
                black_aa = "B02001_003", 
                ai_an = "B02001_004", 
                asian = "B02001_005", 
                hawaiian_pi = "B02001_006", 
                other = "B02001_007"
                ),
  survey = "acs5", #pulls data from 2016-2020 so we have a cross section of 5 years
  year = 2018
) %>%
  filter(str_detect(GEOID, "^36")) %>%
  tibble::add_column(year = 2018)
#2019
acs_ny_2019 <- get_acs(
  geography = "zcta", 
  variables = c(med_income = "B19013_001",
                total_pop = "B01003_001", 
                race_total = "B02001_001", 
                white = "B02001_002", 
                black_aa = "B02001_003", 
                ai_an = "B02001_004", 
                asian = "B02001_005", 
                hawaiian_pi = "B02001_006", 
                other = "B02001_007"
                ),
  survey = "acs5", #pulls data from 2016-2020 so we have a cross section of 5 years
  year = 2019
) %>%
  filter(str_detect(GEOID, "^36")) %>%
  tibble::add_column(year = 2019)
#2020
acs_ny_2020 <- get_acs(
  geography = "zcta", 
  variables = c(med_income = "B19013_001",
                total_pop = "B01003_001", 
                race_total = "B02001_001", 
                white = "B02001_002", 
                black_aa = "B02001_003", 
                ai_an = "B02001_004", 
                asian = "B02001_005", 
                hawaiian_pi = "B02001_006", 
                other = "B02001_007"
                ),
  survey = "acs5", #pulls data from 2016-2020 so we have a cross section of 5 years
  year = 2020
) %>%
  filter(str_detect(GEOID, "^36")) %>%
  tibble::add_column(year = 2020)

acs_ny_comb <- bind_rows(acs_ny_2016, acs_ny_2017, acs_ny_2018, acs_ny_2019, acs_ny_2020) %>% #there are 153 zctas with estimate NA's
  janitor::clean_names() %>%
  write_csv("data/acs_ny_combined.csv")

#  drop_na(estimate) #if we want to drop the 153 NA's but check what the mapping looks like


#acs_ny_comb %>%
#  summarise(across(everything(), ~sum(is.na(.x)))) %>%
#  knitr::kable()
```


**ACS multi year pull through `map_dfr` function**
```{r}
#years interested
years <- lst(2016, 2017, 2018, 2019, 2020) 
#zctas
my_zcta <- c("^ZCTA5 36")
#variables
my_vars <- c(med_income = "B19013_001",
             total_pop = "B01003_001", 
             race_total = "B02001_001", 
             white = "B02001_002", 
             black_aa = "B02001_003", 
             ai_an = "B02001_004", 
             asian = "B02001_005", 
             hawaiian_pi = "B02001_006", 
             other = "B02001_007"
             )

ny_acs <- map_dfr(
  years,
  ~ get_acs(
      geography = "zcta", #idk why there is an error here after designating state and county as NULL
      state = NULL,
      county = NULL,
      variables = my_vars,
      zcta = my_zcta,
      year = .x,
      survey = "acs5"
      ),
  .id = "year"  # when combining results, add id var (name of list item)
  ) %>%
  
  #arrange(variable, NAME) %>% 
  print()
```

**function to get around the zcta geography error**
```{r}
format_variables_acs <- function(variables) {

  # find code in 'data-raw/no_moe_vars.R' to pull these vars from api
  no_moes <- c("B19013_001",
               "B01003_001", 
               "B02001_001", 
               "B02001_002", 
               "B02001_003",
               "B02001_004",
               "B02001_005",
               "B02001_006",
               "B02001_007")

  # First, remove E or M if user has put it in
  variables1 <- map_chr(variables, function(x) {
    if (str_sub(x, -1) %in% c("E", "M")) {
      x <- str_sub(x, 1, -2)
    } else {
      x <- x
    }
  })

  # Now, make unique
  variables2 <- unique(variables1)

  # The comparison profile doesn't have MOEs, so account for that
  if (!any(grepl("^CP[0-9].", variables2))) {

    # Next, separate into vars with and without MOEs
    variables2a <- variables2[!variables2 %in% no_moes]

    variables2_nomoe <- variables2[variables2 %in% no_moes]

    # Now, expand with both E and M if MOE is applicable
    variables3 <- map_chr(variables2a, function(y) paste0(y, c("E", "M"), collapse = ","))

    if (length(variables2_nomoe)) {
      variables3_nomoe <- paste0(variables2_nomoe, "E")

      variables3 <- c(variables3, variables3_nomoe)
    }
  } else {
    variables3 <- paste0(variables2, "E")
  }

  # Now, put together all these strings if need be
  var <- paste0(variables3, collapse = ",")

  return(var)

}

load_data_acs <- function(geography, formatted_variables, key, year, state = NULL,
                          county = NULL, zcta = NULL, survey, show_call = FALSE) {

  base <- paste("https://api.census.gov/data",
                  as.character(year), "acs",
                  survey, sep = "/")

  if (grepl("^DP", formatted_variables)) {
    message("Using the ACS Data Profile")
    base <- paste0(base, "/profile")
  }

  if (grepl("^S[0-9].", formatted_variables)) {
    message("Using the ACS Subject Tables")
    base <- paste0(base, "/subject")
  }

  if (grepl("^CP[0-9].", formatted_variables)) {
    message("Using the ACS Comparison Profile")
    base <- paste0(base, "/cprofile")
  }

  for_area <- paste0(geography, ":*")

  if (!is.null(zcta))  {

    if (is.null(state)) {
      stop("The `zcta` argument requires specifying a state.", call. = FALSE)
    }

    if (!is.null(county)) {
      stop("ZCTAs do not nest within counties, so `county` should not be specified.",
           call. = FALSE)
    }

    state <- map_chr(state, function(x) {
      validate_state(x)
    })

    for_area <- paste0(zcta, collapse = ",")

    in_area <- paste0("state:", state)

    vars_to_get <- paste0(formatted_variables, ",NAME")

    call <- GET(base, query = list(get = vars_to_get,
                                   "for" = paste0(geography, ":", for_area),
                                   "in" = in_area,
                                   key = key))

  } else if (!is.null(state) && is.null(zcta)) {

    state <- map_chr(state, function(x) {
      validate_state(x)
    })

    if (length(state) > 1) {
      state <- paste0(state, collapse = ",")
    }

    if (geography == "state") {
      for_area <- paste0("state:", state)
    }

    if (!is.null(county)) {

      county <- map_chr(county, function(x) {
        validate_county(state, x)
      })

      if (length(county) > 1) {
        county <- paste0(county, collapse = ",")
      }

      if (geography == "county") {

        for_area <- paste0("county:", county)
        in_area <- paste0("state:", state)

      } else {

        in_area <- paste0("state:", state,
                          "+county:", county)

      }

    } else {

      if (geography == "block group" && is.null(county)) {
          in_area <- paste0("state:", state,
                            "&in=county:*")
      } else {
        in_area <- paste0("state:", state)
      }

    }

    vars_to_get <- paste0(formatted_variables, ",NAME")

    if (geography == "state" && !is.null(state)) {

      call <- GET(base, query = list(get = vars_to_get,
                                     "for" = for_area,
                                     key = key))
    } else {

      call <- GET(base, query = list(get = vars_to_get,
                                     "for" = for_area,
                                     "in" = in_area,
                                     key = key))
    }


  } else {

    vars_to_get <- paste0(formatted_variables, ",NAME")

    call <- GET(base, query = list(get = vars_to_get,
                                   "for" = paste0(geography, ":*"),
                                   key = key))
  }

  if (show_call) {
    call_url <- gsub("&key.*", "", call$url)
    message(paste("Census API call:", call_url))
  }

  # Make sure call status returns 200, else, print the error message for the user.
  if (call$status_code != 200) {
    msg <- content(call, as = "text")

    if (grepl("The requested resource is not available", msg)) {
      stop("One or more of your requested variables is likely not available at the requested geography.  Please refine your selection.", call. = FALSE)
    } else {
      stop(sprintf("Your API call has errors.  The API message returned is %s.", msg), call. = FALSE)
    }

  }

  # callStatus <- http_status(call)
  # if (callStatus$reason != "OK") {
  #   message(callStatus$category, " ", callStatus$reason, " ", callStatus$message)
  # } else {
  #   content <- content(call, as = "text")
  # }
  #
  # validate_call(content = content, geography = geography, year = year,
  #               dataset = survey)

  content <- content(call, as = "text")

  if (grepl("You included a key with this request", content)) {
    stop("You have supplied an invalid or inactive API key. To obtain a valid API key, visit https://api.census.gov/data/key_signup.html. To activate your key, be sure to click the link provided to you in the email from the Census Bureau that contained your key.", call. = FALSE)
  }

  dat <- fromJSON(content)

  colnames(dat) <- dat[1,]

  dat <- as_tibble(dat)

  dat <- dat[-1,]

  var_vector <- unlist(strsplit(formatted_variables, split = ","))

  dat[var_vector] <- lapply(dat[var_vector], as.numeric)

  v2 <- c(var_vector, "NAME")

  # Get the geography ID variables
  id_vars <- names(dat)[! names(dat) %in% v2]

  # Paste into a GEOID column
  dat$GEOID <- do.call(paste0, dat[id_vars])

  # Now, remove them
  dat <- dat[, !(names(dat) %in% id_vars)]

  return(dat)

}


load_data_decennial <- function(geography, variables, key, year, sumfile,
                                state = NULL, county = NULL, show_call = FALSE) {


  var <- paste0(variables, collapse = ",")

  if (year == 1990) {
    vars_to_get <- paste0(var, ",ANPSADPI")
  } else {
    vars_to_get <- paste0(var, ",NAME")
  }


```


